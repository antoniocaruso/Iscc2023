@article{LiSurvey2021,
    title = {Image retrieval from remote sensing big data: A survey},
    journal = {Information Fusion},
    volume = {67},
    pages = {94-115},
    year = {2021},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2020.10.008},
%    url = {https://www.sciencedirect.com/science/article/pii/S1566253520303778},
    author = {Yansheng Li and Jiayi Ma and Yongjun Zhang},
    keywords = {Remote sensing (rs) big data, Rs image retrieval methods, Rs image retrieval applications, Evaluation datasets and performance discussion, Future research directions}
}

@ARTICLE{Adao2017,
	author = {Adão, Telmo and Hruška, Jonáš and Pádua, Luís and Bessa, José and Peres, Emanuel and Morais, Raul and Sousa, Joaquim João},
	title = {Hyperspectral imaging: A review on UAV-based sensors, data processing and applications for agriculture and forestry},
	year = {2017},
	journal = {Remote Sensing},
	volume = {9},
	number = {11},
	doi = {10.3390/rs9111110},
%	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034756154&doi=10.3390},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 572; All Open Access, Gold Open Access, Green Open Access}
}


@Article{agriengineering4020029,
AUTHOR = {Yadav, Sargam and Kaushik, Abhishek and Sharma, Mahak and Sharma, Shubham},
TITLE = {Disruptive Technologies in Smart Farming: An Expanded View with Sentiment Analysis},
JOURNAL = {AgriEngineering},
VOLUME = {4},
YEAR = {2022},
NUMBER = {2},
PAGES = {424--460},
% URL = {https://www.mdpi.com/2624-7402/4/2/29},
ISSN = {2624-7402},
ABSTRACT = {Smart Farming (SF) is an emerging technology in the current agricultural landscape. The aim of Smart Farming is to provide tools for various agricultural and farming operations to improve yield by reducing cost, waste, and required manpower. SF is a data-driven approach that can mitigate losses that occur due to extreme weather conditions and calamities. The influx of data from various sensors, and the introduction of information communication technologies (ICTs) in the field of farming has accelerated the implementation of disruptive technologies (DTs) such as machine learning and big data. Application of these predictive and innovative tools in agriculture is crucial for handling unprecedented conditions such as climate change and the increasing global population. In this study, we review the recent advancements in the field of Smart Farming, which include novel use cases and projects around the globe. An overview of the challenges associated with the adoption of such technologies in their respective regions is also provided. A brief analysis of the general sentiment towards Smart Farming technologies is also performed by manually annotating YouTube comments and making use of the pattern library. Preliminary findings of our study indicate that, though there are several barriers to the implementation of SF tools, further research and innovation can alleviate such risks and ensure sustainability of the food supply. The exploratory sentiment analysis also suggests that most digital users are not well-informed about such technologies.},
DOI = {10.3390/agriengineering4020029}
}




@article{de2018agriculture,
  title={Agriculture 4.0: The future of farming technology},
  author={De Clercq, Matthieu and Vats, Anshu and Biel, Alvaro},
  journal={Proceedings of the World Government Summit, Dubai, UAE},
  pages={11--13},
  year={2018},
  publisher={IEEE}
}

@online{mcfadden2023precision,
  title={Precision Agriculture in the Digital Era: Recent Adoption on US Farms},
  author={McFadden, Jonathan and Njuki, Eric and Griffin, Terry},
  year={2023},
  url = {https://www.ers.usda.gov/webdocs/publications/105894/eib-248.pdf?v=9219.8}
}

@article{lu_survey_2020,
	title = {A survey of public datasets for computer vision tasks in precision agriculture},
	volume = {178},
	issn = {01681699},
%	url = {https://linkinghub.elsevier.com/retrieve/pii/S0168169920312709},
	doi = {10.1016/j.compag.2020.105760},
	pages = {105760},
	journaltitle = {Computers and Electronics in Agriculture},
	shortjournal = {Computers and Electronics in Agriculture},
	author = {Lu, Yuzhen and Young, Sierra},
	urldate = {2023-03-13},
	date = {2020-11},
	langid = {english},
}

@article{bouguettaya_deep_2022,
	title = {Deep learning techniques to classify agricultural crops through {UAV} imagery: a review},
	volume = {34},
	issn = {0941-0643, 1433-3058},
%	url = {https://link.springer.com/10.1007/s00521-022-07104-9},
	doi = {10.1007/s00521-022-07104-9},
	pages = {9511--9536},
	number = {12},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Bouguettaya, Abdelmalek and Zarzour, Hafed and Kechida, Ahmed and Taberkit, Amine Mohammed},
	urldate = {2023-03-13},
	date = {2022-06},
	langid = {english},
}

@Article{s21051617,
    AUTHOR = {Safonova, Anastasiia and Guirado, Emilio and Maglinets, Yuriy and Alcaraz-Segura, Domingo and Tabik, Siham},
    TITLE = {Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask R-CNN},
    JOURNAL = {Sensors},
    VOLUME = {21},
    YEAR = {2021},
    NUMBER = {5},
    ARTICLE-NUMBER = {1617},
  %  URL = {https://www.mdpi.com/1424-8220/21/5/1617},
    PubMedID = {33668984},
    ISSN = {1424-8220},
    DOI = {10.3390/s21051617},
    keywords={rel_wor}
}

@ARTICLE{Egli20201,
	author = {Egli, Sebastian and Höpke, Martin},
	title = {Cnn-based tree species classification using high resolution rgb image data from automated uav observations},
	year = {2020},
	journal = {Remote Sensing},
	volume = {12},
	number = {23},
	pages = {1 – 17},
	doi = {10.3390/rs12233892},
%	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097031088&doi=10.3390%2frs12233892&partnerID=40&md5=9437792d192fe00c9cf06bf07a385a40},
	type = {Article},
	keywords = {rel_wor},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access, Gold Open Access, Green Open Access}
}

@Article{rs14061523,
    AUTHOR = {Ye, Zhangxi and Wei, Jiahao and Lin, Yuwei and Guo, Qian and Zhang, Jian and Zhang, Houxi and Deng, Hui and Yang, Kaijie},
    TITLE = {Extraction of Olive Crown Based on UAV Visible Images and the U2-Net Deep Learning Model},
    JOURNAL = {Remote Sensing},
    VOLUME = {14},
    YEAR = {2022},
    NUMBER = {6},
    ARTICLE-NUMBER = {1523},
  %  URL = {https://www.mdpi.com/2072-4292/14/6/1523},
    ISSN = {2072-4292},
    DOI = {10.3390/rs14061523},
    keywords={rel_wor}
}

@article{ruswurm_multi-temporal_2018,
	title = {Multi-{Temporal} {Land} {Cover} {Classification} with {Sequential} {Recurrent} {Encoders}},
	volume = {7},
	issn = {2220-9964},
%	url = {http://arxiv.org/abs/1802.02080},
	doi = {10.3390/ijgi7040129},
	abstract = {Earth observation (EO) sensors deliver data at daily or weekly intervals. Most land use and land cover classiﬁcation (LULC) approaches, however, are designed for cloud-free and mono-temporal observations. The increasing temporal capabilities of today’s sensors enable the use of temporal, along with spectral and spatial features.Domains such as speech recognition or neural machine translation, work with inherently temporal data and, today, achieve impressive results by using sequential encoder-decoder structures. Inspired by these sequence-to-sequence models, we adapt an encoder structure with convolutional recurrent layers in order to approximate a phenological model for vegetation classes based on a temporal sequence of Sentinel 2 (S2) images. In our experiments, we visualize internal activations over a sequence of cloudy and non-cloudy images and ﬁnd several recurrent cells that reduce the input activity for cloudy observations. Hence, we assume that our network has learned cloud-ﬁltering schemes solely from input data, which could alleviate the need for tedious cloud-ﬁltering as a preprocessing step for many EO approaches. Moreover, using unﬁltered temporal series of top-of-atmosphere (TOA) reﬂectance data, our experiments achieved state-of-the-art classiﬁcation accuracies on a large number of crop classes with minimal preprocessing, compared to other classiﬁcation approaches.},
	language = {en},
	number = {4},
	urldate = {2022-06-15},
	journal = {ISPRS International Journal of Geo-Information},
	author = {Rußwurm, Marc and Körner, Marco},
	year = {2018},
	note = {arXiv:1802.02080 [cs]},
	keywords = {sat},
	pages = {129},
}

@misc{daudt_fully_2018,
	title = {Fully {Convolutional} {Siamese} {Networks} for {Change} {Detection}},
	% url = {http://arxiv.org/abs/1810.08462},
	abstract = {This paper presents three fully convolutional neural network architectures which perform change detection using a pair of coregistered images. Most notably, we propose two Siamese extensions of fully convolutional networks which use heuristics about the current problem to achieve the best results in our tests on two open change detection datasets, using both RGB and multispectral images. We show that our system is able to learn from scratch using annotated change detection images. Our architectures achieve better performance than previously proposed methods, while being at least 500 times faster than related systems. This work is a step towards efﬁcient processing of data from large scale Earth observation systems such as Copernicus or Landsat.},
	language = {en},
	urldate = {2022-07-12},
	publisher = {arXiv},
	author = {Daudt, Rodrigo Caye and Saux, Bertrand Le and Boulch, Alexandre},
	month = oct,
	year = {2018},
	note = {Number: arXiv:1810.08462 arXiv:1810.08462 [cs]},
	keywords = {sat},
%	file = {Daudt et al. - 2018 - Fully Convolutional Siamese Networks for Change De.pdf:/home/lore/Zotero/storage/L4UP7FIB/Daudt et al. - 2018 - Fully Convolutional Siamese Networks for Change De.pdf:application/pdf},
}

@article{ienco_land_2017,
	title = {Land {Cover} {Classification} via {Multi}-temporal {Spatial} {Data} by {Recurrent} {Neural} {Networks}},
	volume = {14},
	issn = {1545-598X, 1558-0571},
%	url = {http://arxiv.org/abs/1704.04055},
	doi = {10.1109/LGRS.2017.2728698},
	abstract = {Nowadays, modern earth observation programs produce huge volumes of satellite images time series (SITS) that can be useful to monitor geographical areas through time. How to efﬁciently analyze such kind of information is still an open question in the remote sensing ﬁeld. Recently, deep learning methods proved suitable to deal with remote sensing data mainly for scene classiﬁcation (i.e. Convolutional Neural Networks - CNNs - on single images) while only very few studies exist involving temporal deep learning approaches (i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.},
	language = {en},
	number = {10},
	urldate = {2022-06-15},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Ienco, Dino and Gaetano, Raffaele and Dupaquier, Claire and Maurel, Pierre},
	month = oct,
	year = {2017},
	note = {arXiv:1704.04055 [cs]},
	keywords = {sat},
	pages = {1685--1689},
	file = {Ienco et al. - 2017 - Land Cover Classification via Multi-temporal Spati.pdf:/home/lore/Zotero/storage/AMTHB9DV/Ienco et al. - 2017 - Land Cover Classification via Multi-temporal Spati.pdf:application/pdf},
}

@misc{gurumurthy_mango_2019,
	title = {Mango {Tree} {Net} -- {A} fully convolutional network for semantic segmentation and individual crown detection of mango trees},
	% url = {http://arxiv.org/abs/1907.06915},
	abstract = {This work presents a method for semantic segmentation of mango trees in high resolution aerial imagery, and, a novel method for individual crown detection of mango trees using segmentation output. Mango Tree Net, a fully convolutional neural network (FCN), is trained using supervised learning to perform semantic segmentation of mango trees in imagery acquired using an unmanned aerial vehicle (UAV). The proposed network is retrained to separate touching/overlapping tree crowns in segmentation output. Contour based connected object detection is performed on the segmentation output from retrained network. Bounding boxes are drawn on the original images using coordinates of connected objects to achieve individual crown detection. The training dataset consists of 8, 824 image patches of size 240 × 240. The approach is tested for performance on segmentation and individual crown detection tasks using test datasets containing 36 and 4 images respectively. The performance is analyzed using standard metrics precision, recall, f1-score and accuracy. Results obtained demonstrate the robustness of the proposed methods despite variations in factors such as scale, occlusion, lighting conditions and surrounding vegetation.},
	language = {en},
	urldate = {2022-10-26},
	publisher = {arXiv},
	author = {Gurumurthy, Vikas Agaradahalli and Kestur, Ramesh and Narasipura, Omkar},
	month = jul,
	year = {2019},
	note = {arXiv:1907.06915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, rel_wor},
	file = {Gurumurthy et al. - 2019 - Mango Tree Net -- A fully convolutional network fo.pdf:/home/lore/Zotero/storage/95RHBGYH/Gurumurthy et al. - 2019 - Mango Tree Net -- A fully convolutional network fo.pdf:application/pdf},
}

@article{guirado_mask_2021,
	title = {Mask {R}-{CNN} and {OBIA} {Fusion} {Improves} the {Segmentation} of {Scattered} {Vegetation} in {Very} {High}-{Resolution} {Optical} {Sensors}},
	volume = {21},
	issn = {1424-8220},
%	url = {https://www.mdpi.com/1424-8220/21/1/320},
	doi = {10.3390/s21010320},
	abstract = {Vegetation generally appears scattered in drylands. Its structure, composition and spatial patterns are key controls of biotic interactions, water, and nutrient cycles. Applying segmentation methods to very high-resolution images for monitoring changes in vegetation cover can provide relevant information for dryland conservation ecology. For this reason, improving segmentation methods and understanding the effect of spatial resolution on segmentation results is key to improve dryland vegetation monitoring. We explored and analyzed the accuracy of Object-Based Image Analysis (OBIA) and Mask Region-based Convolutional Neural Networks (Mask R-CNN) and the fusion of both methods in the segmentation of scattered vegetation in a dryland ecosystem. As a case study, we mapped Ziziphus lotus, the dominant shrub of a habitat of conservation priority in one of the driest areas of Europe. Our results show for the ﬁrst time that the fusion of the results from OBIA and Mask R-CNN increases the accuracy of the segmentation of scattered shrubs up to 25\% compared to both methods separately. Hence, by fusing OBIA and Mask R-CNNs on very high-resolution images, the improved segmentation accuracy of vegetation mapping would lead to more precise and sensitive monitoring of changes in biodiversity and ecosystem services in drylands.},
	language = {en},
	number = {1},
	urldate = {2022-10-26},
	journal = {Sensors},
	author = {Guirado, Emilio and Blanco-Sacristán, Javier and Rodríguez-Caballero, Emilio and Tabik, Siham and Alcaraz-Segura, Domingo and Martínez-Valderrama, Jaime and Cabello, Javier},
	month = jan,
	year = {2021},
	keywords={rel_wor},
	pages = {320},
	file = {Guirado et al. - 2021 - Mask R-CNN and OBIA Fusion Improves the Segmentati.pdf:/home/lore/Zotero/storage/KCYZYH5N/Guirado et al. - 2021 - Mask R-CNN and OBIA Fusion Improves the Segmentati.pdf:application/pdf},
}


@misc{ronneberger_u-net_2015,
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
%	url = {http://arxiv.org/abs/1505.04597},
	shorttitle = {U-Net},
	number = {{arXiv}:1505.04597},
	publisher = {{arXiv}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	urldate = {2022-07-13},
	date = {2015-05-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:/home/lore/Zotero/storage/XMPDRWWX/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}
